{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aq5jjKHb-zdZ"
   },
   "source": [
    "# <u> Problem 1</u>\n",
    "\n",
    "### Write a function which takes the excel column name as an input and returns the corresponding column number. A few examples are :\n",
    "\n",
    "* column name = <code>'J'</code> , column number = <code>10</code>\n",
    "* column name = <code>'AP'</code> , column number = <code>42</code>\n",
    "* column name = <code>'AAA'</code>, column number = <code>703</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: “AAA” → 703\n",
    "# \t•\t‘A’ = 1 → 1 × 26^2 = 676\n",
    "# \t•\t‘A’ = 1 → 1 × 26^1 = 26\n",
    "# \t•\t‘A’ = 1 → 1 × 26^0 = 1\n",
    "# \t•\tTotal = 676 + 26 + 1 = 703 ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "Un0QXOa3-zdb"
   },
   "outputs": [],
   "source": [
    "def get_excel_column_number(column_name):\n",
    "  '''\n",
    "  This functions returns the corresponding column number for an excel column name\n",
    "  '''\n",
    "  # Write your code here\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "VFSrFUwP-zdd"
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "get_excel_column_number('BD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "AZaQlyAc-zde"
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "get_excel_column_number('ALQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zh_MA_BN-zd5"
   },
   "source": [
    "# <u> Problem 2</u>\n",
    "\n",
    "### We evaluate a standard machine learning classification model using various evaluation metrics. A classification model is a model which classifies a given observation or an event to a fixed set of categories. Suppose I train a machine learning model to classify images of cats and dogs. For each image, the machine classifies the image with either a <code>'Cat'</code> or a <code>'Dog'</code>. So in essence for each input image, there is a corresponding output by the model. This output can either be a <code>'Cat'</code> or a <code>'Dog'</code>.\n",
    "\n",
    "### To evaluate such a machine learning model which is trained to classify a given observation with at most two labels, a lot of candidate evaluation metrics are available. Accuracy is one of such evaluation metrics.\n",
    "\n",
    "### <u> Accuracy defintion </u> : Suppose you are given 20 input images of cats and dogs. You already know from these images that there are 11 cats and 9 dogs. You train a machine learning model to classify these images into cats and dogs. The machine predicts 9 cats correctly and 8 dogs correctly. The accuracy of the model is then defined as (correctly predicted cats and dogs)/(total cats and dogs). In this case it is (9+8)/(11+9) = 17/20 = 0.85 or 85%. It is a good practice to report the accuracy in percentages\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cp2SUazju_D7"
   },
   "source": [
    "### You are given two lists each of length 20 : one list contains the actual labels for the images of cats and dogs. The other list contains the predicted labels (by the machine) for the images of cats and dogs.\n",
    "\n",
    "### How to read the two lists ? For example for the first image, actual label is Cat and the predicted label is Cat. For the last image, actual label is Dog and the predicted label is Cat. The same index in the two lists corresponds to the same image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "BPsQbNPjvjAT"
   },
   "outputs": [],
   "source": [
    "actual_labels = ['Cat','Dog','Cat','Cat','Dog','Cat','Dog','Cat','Cat','Cat','Dog','Dog','Cat','Cat','Cat','Dog','Dog','Dog','Cat','Dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "MDefeYf1v4pT"
   },
   "outputs": [],
   "source": [
    "predicted_labels = ['Cat','Dog','Cat','Cat','Dog','Cat','Dog','Cat','Dog','Dog','Dog','Dog','Cat','Cat','Cat','Dog','Dog','Dog','Cat','Cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzLROuvWwZAL"
   },
   "source": [
    "### Write a function to calculate the accuracy. This functions takes two lists as inputs and returns the accuracy score in percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "HvDJXfL3-zd6"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(actual, predicted):\n",
    "    \"\"\"\n",
    "    This function calculates the accuracy based on two input lists.\n",
    "    \"\"\"\n",
    "    total_accurate = 0  \n",
    "\n",
    "    for i in range(len(actual)):  \n",
    "        if predicted[i] == actual[i]:  \n",
    "            total_accurate += 1  \n",
    "\n",
    "    return (total_accurate / len(actual))*100  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "At0m0hcc-zd_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the accuracy score for the given lists\n",
    "calculate_accuracy(actual_labels,predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyYwXAlax36r"
   },
   "source": [
    "* ### <b><u>Precision</u></b> for cats is defined as the number of correctly predicted cats divided by the number of predicted cats. Report precision in percentages.\n",
    "\n",
    "* ### <b><u>Recall</u></b> for cats is defined as the number of correctly predicted cats divided by the actual number of cats. Report recall in percentages.\n",
    "\n",
    "### We can define the same two metrics for dogs as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwYIek-9yd4n"
   },
   "source": [
    "# <u> Problem 3 </u>\n",
    "\n",
    "### Write a Python function which returns the precision and recall for a given input label. Use the same two lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "correctly_predicted_cats = 0\n",
    "\n",
    "for i in range(len(actual_labels)):\n",
    "    if actual_labels[i] == \"Cat\" and predicted_labels[i] == \"Cat\":\n",
    "        correctly_predicted_cats += 1\n",
    "\n",
    "print(correctly_predicted_cats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "SojiZTMSyY4T"
   },
   "outputs": [],
   "source": [
    "def precision_recall(label = 'Dog'):\n",
    "    correctly_predicted_cats = 0\n",
    "    \n",
    "    for i in range(len(actual_labels)):\n",
    "        if actual_labels[i] == \"Dog\" and predicted_labels[i] == \"Dog\":\n",
    "            correctly_predicted_cats += 1\n",
    "\n",
    "    precision =  correctly_predicted_cats / predicted_labels.count('Dog')\n",
    "\n",
    "    recall = correctly_predicted_cats / actual_labels.count('Dog')\n",
    "\n",
    "    return precision,recall\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8, 0.8888888888888888)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall(label='Dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-5eBWXa1Mla"
   },
   "source": [
    "# <u> Problem 4 </u>\n",
    "\n",
    "### Write a Python function which takes a sentence and a length value as inputs and returns the counts of those words from the sentence whose length is equal to the provided input length value.\n",
    "\n",
    "### Suppose if the input for the length value is 5, it will return the count of all those words which are of length 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "9ueu9nJx1uUZ"
   },
   "outputs": [],
   "source": [
    "marvel_quote = \"The world has changed and none of us can go back. All we can do is our best, and sometimes the best that we can do is to start over.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "marvel_quote_split = marvel_quote.replace(\".\",\"\").replace(\",\",\"\").split(\" \")\n",
    "# marvel_quote_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "9OPAUOCT2VQW"
   },
   "outputs": [],
   "source": [
    "# Your function below. Make sure to remove the special characters such as full stop and comma.\n",
    "def fixed_length_word_counts(sentence, length=3):\n",
    "    split_sentence = sentence.replace(\".\",\"\").replace(\",\",\"\").split(\" \")\n",
    "    count_length_word = 0\n",
    "    for word in split_sentence:\n",
    "        if len(word) == length:\n",
    "            count_length_word += 1\n",
    "\n",
    "    return count_length_word\n",
    "            \n",
    "            \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "Px7OQG0j2iab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check on the sample sentence\n",
    "fixed_length_word_counts(marvel_quote,length=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1A3RAt8Cg866gxSBsGYE5SH0yBpDuUHPi",
     "timestamp": 1738213133093
    },
    {
     "file_id": "1xNU5OXxishKoye1nOgEM0Jpkr0ZZi1Zt",
     "timestamp": 1603129985847
    },
    {
     "file_id": "1emDbVKr-LXyURvde4_3SRMnJgqm37xoR",
     "timestamp": 1603129797361
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
